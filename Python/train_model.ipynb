{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 11:08:37.005663: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 11:08:38.120320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 11:08:40.505945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-30 11:08:40.542503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-30 11:08:40.542634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from config import Config\n",
    "sys.path.append(Config.root_path)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Load Model\n",
    "from Python.model.segmentation_model import deeplabv3plus\n",
    "# Load Dataset & Preprocessing\n",
    "from Python.data_processing.utils import get_png_paths_from_dir\n",
    "from Python.data_processing.utils import load_weight_map\n",
    "from Python.data_processing.data_generator import generate_image_dataset_from_files\n",
    "from Python.data_processing.data_generator import split_dataset\n",
    "from Python.data_processing.data_generator import augment_dataset\n",
    "# Print Model Prediction\n",
    "from Python.data_processing.compare_predictions import compare_model_predictions\n",
    "# Save Model\n",
    "from Python.legacy.utils_old import write_model_to_disk\n",
    "\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "different number of image and mask files, found 508image files and 0 mask files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m weight_map\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m      6\u001b[0m     weights[\u001b[39mint\u001b[39m(key)] \u001b[39m=\u001b[39m weight_map\u001b[39m.\u001b[39mget(key)\n\u001b[0;32m----> 8\u001b[0m dataset \u001b[39m=\u001b[39m generate_image_dataset_from_files(\n\u001b[1;32m      9\u001b[0m     image_files, \n\u001b[1;32m     10\u001b[0m     mask_files, \n\u001b[1;32m     11\u001b[0m     Config\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m     12\u001b[0m     tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE, \n\u001b[1;32m     13\u001b[0m     Config\u001b[39m.\u001b[39;49mshuffle_size, \n\u001b[1;32m     14\u001b[0m     weights\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m train_dataset, val_dataset, test_dataset \u001b[39m=\u001b[39m split_dataset(\n\u001b[1;32m     17\u001b[0m     dataset, Config\u001b[39m.\u001b[39mtrain_size, Config\u001b[39m.\u001b[39mval_size, Config\u001b[39m.\u001b[39mtest_size\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m train_dataset \u001b[39m=\u001b[39m augment_dataset(train_dataset)\n",
      "File \u001b[0;32m/mnt/d/Liws/OneDrive - University of Bristol/Linux/CEZ_Mapping/Python/data_processing/data_generator.py:83\u001b[0m, in \u001b[0;36mgenerate_image_dataset_from_files\u001b[0;34m(image_files, mask_files, batch_size, prefetch, shuffle_size, weights)\u001b[0m\n\u001b[1;32m     81\u001b[0m     errmsg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdifferent number of image and mask files, found \u001b[39m\u001b[39m{\u001b[39;00mn_images\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     errmsg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimage files and \u001b[39m\u001b[39m{\u001b[39;00mn_masks\u001b[39m}\u001b[39;00m\u001b[39m mask files\u001b[39m\u001b[39m\"\u001b[39m \n\u001b[0;32m---> 83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(errmsg)\n\u001b[1;32m     85\u001b[0m weights \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_weight_mask\u001b[39m(\n\u001b[1;32m     88\u001b[0m     image_array: tf\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m     89\u001b[0m     mask_array: tf\u001b[39m.\u001b[39mTensor\n\u001b[1;32m     90\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[tf\u001b[39m.\u001b[39mTensor, tf\u001b[39m.\u001b[39mTensor, tf\u001b[39m.\u001b[39mTensor]:\n",
      "\u001b[0;31mValueError\u001b[0m: different number of image and mask files, found 508image files and 0 mask files"
     ]
    }
   ],
   "source": [
    "image_files = get_png_paths_from_dir(Config.image_path)\n",
    "mask_files = get_png_paths_from_dir(Config.segmentation_path)\n",
    "weight_map = load_weight_map(Config.weight_map_path)\n",
    "weights = np.zeros(Config.output_channels, dtype=float)\n",
    "for key in weight_map.keys():\n",
    "    weights[int(key)] = weight_map.get(key)\n",
    "\n",
    "dataset = generate_image_dataset_from_files(\n",
    "    image_files, \n",
    "    mask_files, \n",
    "    Config.batch_size,\n",
    "    tf.data.AUTOTUNE, \n",
    "    Config.shuffle_size, \n",
    "    weights\n",
    ")\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(\n",
    "    dataset, Config.train_size, Config.val_size, Config.test_size\n",
    ")\n",
    "train_dataset = augment_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 11:06:49.322233: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-30 11:06:49.322412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-30 11:06:49.322460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-30 11:06:51.543220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-30 11:06:51.543494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-30 11:06:51.543510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-30 11:06:51.543586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-30 11:06:51.543783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3858 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = deeplabv3plus(\n",
    "  Config.input_shape, \n",
    "  Config.batch_size, \n",
    "  Config.output_channels,\n",
    "  Config.channels_low,\n",
    "  Config.channels_high,\n",
    "  Config.middle_repeat\n",
    ")\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]\n",
    "\n",
    "\n",
    "def show_predictions(model, dataset=None, num=1):\n",
    "  for image, mask, weight in iter(dataset.take(num)):\n",
    "    compare_model_predictions(model, image[0], mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m show_predictions(model, dataset, num\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "show_predictions(model, dataset, num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit(\n",
    "  train_dataset, \n",
    "  epochs=20,\n",
    "  validation_data=val_dataset,\n",
    "  shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
    "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_model_to_disk(model, model_history, Config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

import sys
import tensorflow as tf
import numpy as np
from functools import reduce
from Python.config import Config


def get_weights(seg_array: np.ndarray) -> np.ndarray:
    unique, counts = np.unique(seg_array, return_counts=True)
    multiply = lambda x, y : x * y
    n_pixels = reduce(multiply, seg_array.shape)
    n_categories = len(unique)
    category_weights = n_pixels / (n_categories * counts)
    background_index = np.nonzero(unique == Config.background_label)
    category_weights[background_index] = 0
    weight_array = np.zeros(seg_array.shape, dtype=np.float32)
    for key, value in zip(unique, category_weights):
        weight_array[seg_array == key] = value
    return weight_array


def create_dataset(
    image_array: np.ndarray,
    seg_array: np.ndarray,
    weight_array: np.ndarray = np.array([None, None]),
    normalise_images: bool = True
) -> tf.data.Dataset:
    """Create a normalised dataset from image and segmentation mask arrays.
    
    Args:
        image_array (np.ndarray) : Array of images in order image, row, 
            column, pixel.
        seg_array (np.ndarray) : Array of segmentation masks in order mask, 
            row, column, pixel.
        weight_array (np.ndarray | bool) : Array of values that weight the 
            segmentation mask.
        normalise_images (bool) : Set to true to normalise pixel values from 
            (0, 255) to (0, 1).
    
    Returns:
        dataset (tf.data.Dataset) : Image dataset returning (images, masks).
    """ 
    # Check first dimension of image_arr and segmentaiton_arr match
     
    if (n_image := image_array.shape[0]) != (n_seg := seg_array.shape[0]):
        errmsg = "image_array and seg_array must contain same number of images"
        errmsg += f"but image_array contains {n_image} images and seg_array "
        errmsg += f"contains {n_seg} images."
        raise ValueError(errmsg)

    if normalise_images:
        image_array = image_array / 255
    image_array = image_array.astype(np.float32)
    seg_array = seg_array.astype(np.float32)
    if (weight_array != None).any():
        weight_array = weight_array.astype(np.float32)
        dataset = tf.data.Dataset.from_tensor_slices(
            (image_array, seg_array, weight_array), name="dataset"
        )
    else: 
        dataset = tf.data.Dataset.from_tensor_slices(
            (image_array, seg_array), name="dataset"
        )
    return dataset


def expand_dataset(dataset: tf.data.Dataset, batch_size: int) -> tf.data.Dataset:
    """Increase the size of a dataset with synthetic data.
    
    The syntheic data is generated by applying augmentations to the input data.
    These augmentations contain all combindations of x, y flips and all 90 
    degree rotations.
    
    Args:
        dataset (tf.data.Dataset) : The dataset to expand which must return 
        (images, masks).
        batch_size (int) : Number of datapoints per batch.
    Returns:
        dataset_expanded (tf.data.Dataset) : The expanded dataset."""
    sections = tuple(zip(*dataset))
    sections_new = []
    for section in sections:
        elements_new = []
        for element in section:
            flipped_lr = tf.image.flip_left_right(element)
            flipped_ud = tf.image.flip_up_down(element)
            flipped_lrud = tf.image.flip_up_down(flipped_lr)
            rotate_dataset = lambda x : [
                tf.image.rot90(x, k=angle) for angle in range(3)
            ]
            element_rotations = rotate_dataset(element)
            flipped_lr_rotations = rotate_dataset(flipped_lr)
            flipped_ud_rotations = rotate_dataset(flipped_ud)
            flipped_lrud_rotations = rotate_dataset(flipped_lrud)
            elements_new.extend([
                element, 
                flipped_lr, 
                flipped_ud,
                flipped_lrud, 
                *element_rotations, 
                *flipped_lr_rotations, 
                *flipped_ud_rotations,
                *flipped_lrud_rotations
            ])
        sections_new.append(tf.convert_to_tensor(elements_new))
    # tf.data.Dataset.from_tensor_slices((*sections_new))
    # dataset_expanded = tf.data.Dataset.from_tensors(new_batches)
    # dataset_expanded = dataset_expanded.unbatch()
    dataset_expanded = dataset_expanded.batch(batch_size)

def _expand_dataset_tensors(image: tf.Tensor, mask: tf.Tensor, weight: tf.Tensor):
    elements_new = []
    for element in (image, mask, weight):
        flipped_lr = tf.image.flip_left_right(element)
        flipped_ud = tf.image.flip_up_down(element)
        flipped_lrud = tf.image.flip_up_down(flipped_lr)
        rotate_dataset = lambda x : [
            tf.image.rot90(x, k=angle) for angle in range(3)
        ]
        element_rotations = rotate_dataset(element)
        flipped_lr_rotations = rotate_dataset(flipped_lr)
        flipped_ud_rotations = rotate_dataset(flipped_ud)
        flipped_lrud_rotations = rotate_dataset(flipped_lrud)
        elements_new.append(
            tf.convert_to_tensor(
                [
                    element, 
                    flipped_lr, 
                    # flipped_ud,
                    # flipped_lrud, 
                    # *element_rotations#,
                    # *flipped_lr_rotations, 
                    # *flipped_ud_rotations,
                    # *flipped_lrud_rotations
                ]
            )
        )
    dataset = tf.data.Dataset.from_tensor_slices(tuple(elements_new))
    return dataset
    

def expand_dataset_v2(dataset: tf.data.Dataset):
    n_images = len(dataset)
    expansion_coeff = Config.expansion_coeff
    dataset = dataset.interleave(_expand_dataset_tensors)
    dataset = dataset = dataset.apply(
        tf.data.experimental.assert_cardinality(n_images * expansion_coeff)
    )
    return dataset


def split_dataset(
    dataset: tf.data.Dataset,
    train_size: float,
    val_size: float,
    test_size: float
) -> tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:
    """Partitions a dataset into train validation and test sets.
    
    Args:
        dataset (tf.data.Dataset) : The dataset to split. This must return
            (image, mask) when iterated across.
        train_size (float) : Fraction of dataset to use for training.
        val_size (float) : Fraction of dataset to use for validation.
        test_size (float) : Fraction of dataset to use for testing.
    
    Returns:
        train_dataset (tf.data.Dataset) : The dataset to use for training.
        val_dataset (tf.data.Dataset) : The dataset to use for validation.
        test_dataset (tf.data.Dataset) : The dataset to use for testing.
    """
    dataset_size = len(dataset)
    sum_partitions = train_size + val_size, test_size
    if np.allclose(sum_partitions, 1, atol=3*sys.float_info.epsilon):
        errmsg = f"Paritions should sum to 1 but sum to {sum_partitions}"
        raise ValueError(errmsg)
    
    train_size = int(dataset_size * train_size)
    val_size = int(dataset_size * val_size)
    test_size = int(dataset_size * test_size)
    train_dataset = dataset.take(train_size)
    val_dataset = dataset.skip(train_size)
    test_dataset = val_dataset.skip(val_size)
    val = val_dataset.take(val_size)
    return train_dataset, val_dataset, test_dataset


def data_pipeline(
    image_array: np.ndarray,
    seg_array: np.ndarray,
    train_size: float,
    val_size: float,
    test_size: float,
    normalise_images: bool = False
) -> tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:
    """Create normalised, batched datasets partitioned into train, validation, 
    and test sets. These sets are expanded with synthetic data generated by 
    applying all possible combinations of x, y flips and 90 degree rotations.
    
    Args:
        image_array (np.ndarray) : Input images.
        seg_array (np.ndarray) : Segmentation masks.
        batch_size (int) : Number of images to combine into a batch.
        train_size (float) : Fraction of the data to use for training.
        val_size (float) : Fraction of the data to use for validation.
        test_size (float) : Fraction of the data to use for testing.
        image_normalised (bool) : Set to true if image_array contains 
            images with pixel values normalised between (0, 1). 
    
    Returns:
        train_dataset(tf.data.Dataset) : Dataset to use for training.
        val_dataset(tf.data.Dataset) : Dataset to use for validation.
        test_dataset(tf.data.Dataset) : Dataset to use for testing.
    """
    print("Generating weights")
    weight_array = get_weights(seg_array)
    print("Complete")
    dataset = create_dataset(
        image_array, 
        seg_array,
        weight_array=weight_array, 
        normalise_images=normalise_images
    )
    print("Generating sythetic data")
    dataset = expand_dataset_v2(dataset)
    print("Complete")
    print("Shuffling data")
    dataset = dataset.shuffle(Config.shuffle_size)
    print("Complete")
    dataset = dataset.batch(Config.batch_size, drop_remainder=True)
    train_dataset, val_dataset, test_dataset = split_dataset(
        dataset, train_size, val_size, test_size
    )
    return train_dataset, val_dataset, test_dataset